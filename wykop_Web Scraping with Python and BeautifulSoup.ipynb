{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from mtranslate import translate\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "from time import time\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "Reference: https://www.dataquest.io/blog/web-scraping-beautifulsoup/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Wykop for all posts with the mention of \"smog\"\n",
    "\n",
    "posts = []\n",
    "votes = []\n",
    "dates = []\n",
    "images = []\n",
    "users = []\n",
    "\n",
    "start_time = time()\n",
    "request_count = 0\n",
    "req_sess = requests.Session()\n",
    "\n",
    "for page_num in range(1, 163):\n",
    "# for page_num in enumerate(response): #I believe this method will scrape not just posts but also comments\n",
    "\n",
    "    response = req_sess.get(f\"https://www.wykop.pl/szukaj/wpisy/smog/strona/{page_num}/\")\n",
    "\n",
    "    # Pause the loop\n",
    "    #sleep(randint(1,3))\n",
    "\n",
    "    # Monitor the requests\n",
    "    request_count += 1\n",
    "    elapsed_time = time() - start_time\n",
    "    print('Page {}; Request:{}; Frequency: {} requests/s'.format(page_num, request_count, request_count/elapsed_time))\n",
    "\n",
    "    #clear_output(wait = True)\n",
    "    # Throw a warning for non-200 status codes\n",
    "    if response.status_code != 200:\n",
    "        print('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "        print(response.headers)\n",
    "\n",
    "    # Break the loop if the number of requests is greater than expected\n",
    "    #if requests > 10:\n",
    "    #    print('Number of requests was greater than expected.')\n",
    "    #    break\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    results = soup.find_all('li', class_=\"entry iC\")\n",
    "\n",
    "    for result in results:\n",
    "        # Error handling\n",
    "        try:\n",
    "            post = result.find('div', class_=\"text\").text\n",
    "            posts.append(post)\n",
    "\n",
    "            date = result.time['title']\n",
    "            dates.append(date)\n",
    "\n",
    "            vote = result.p.b.span.text\n",
    "            vote = int(vote)\n",
    "            votes.append(vote)\n",
    "\n",
    "            user = result.div.b.text\n",
    "            users.append(user)\n",
    "\n",
    "            image = result.find('img',class_='block lazy')\n",
    "            images.append(image)\n",
    "\n",
    "        except AttributeError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wykopDF1 = pd.DataFrame({'post':posts,\n",
    "                       'date': dates,\n",
    "                         'user':users,\n",
    "                       'vote': votes,\n",
    "                        'image':images\n",
    "})\n",
    "# wykopDF1.to_csv('rawWebScraps_Wykop/wykopRawV2.csv')\n",
    "print(wykopDF1.info())\n",
    "\n",
    "wykopDF1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "<b>Note</b>: I'm removing \"html residual\" elements manually such as \\n and \\t.\n",
    "<br><br>\n",
    "<b>Note #2</b>: Because I ran the script on two separate occassions (March and May 2019), I am concatenating both dataframes and dropping duplicated rows (i.e. scraped posts) from the May scraping.\n",
    "<br><br>\n",
    "<b>Note #3</b>: Couldn't exactly figure out how to loop the google translation module (mtranslate) to translate all posts from Polish to English into a new column :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mtranslate import translate as mtranslate\n",
    "import re\n",
    "\n",
    "data = pd.read_csv(\"rawWebScraps_Wykop/wykopRaw.csv\")\n",
    "data2 = pd.read_csv(\"rawWebScraps_Wykop/wykopRawMay2019.csv\")\n",
    "\n",
    "\n",
    "del data['Unnamed: 0']\n",
    "data['post'] = data['post'].str.strip()\n",
    "data['post'] = data['post'].replace(r'\\n', '', regex = True)\n",
    "data['post'] = data['post'].replace(r'\\t', '', regex = True)\n",
    "del data2['Unnamed: 0']\n",
    "data2['post'] = data2['post'].str.strip()\n",
    "data2['post'] = data2['post'].replace(r'\\n', '', regex = True)\n",
    "data2['post'] = data2['post'].replace(r'\\t', '', regex = True)\n",
    "# data['postEn'] = ''\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data2['date'] = pd.to_datetime(data2['date'])\n",
    "\n",
    "\n",
    "data2.tail() #7288\n",
    "data.tail()  #7287\n",
    "\n",
    "dataDF = pd.concat([data,data2]).drop_duplicates().reset_index(drop=True)\n",
    "dataDF = dataDF.sort_values(by = 'date')\n",
    "dataDF['Month-Year']=dataDF.date.apply(lambda x: str(x)[:7])\n",
    "dataDF.to_csv('rawWebScraps_Wykop/wykop2012_18.csv')\n",
    "dataDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF = pd.read_csv(\"raw/wykop2012_18.csv\")\n",
    "del dataDF['Unnamed: 0']\n",
    "dataDF['date'] = pd.to_datetime(dataDF['date'])\n",
    "\n",
    "polair = pd.read_csv(\"raw/airPoland2017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter DF to Jan. 2017\n",
    "mask = (dataDF['Month-Year'] == '2017-01')\n",
    "Jan2017 = dataDF.loc[mask]\n",
    "sum(Jan2017.post.str.len())\n",
    "Jan2017['Days'] = Jan2017.date.apply(lambda x: str(x)[8:10])\n",
    "Jan2017['Krakowcount'] = Jan2017.post.str.count('krakow')\n",
    "Jan2017['Warszawacount'] = Jan2017.post.str.count('warszawa')\n",
    "\n",
    "krakowCount = Jan2017.resample('D', on='date').sum().reset_index()\n",
    "WarszawaCount = Jan2017.resample('D', on='date').sum().reset_index()\n",
    "WarszawaCount\n",
    "# Jan2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jan2017_hash_count = pd.DataFrame(Jan2017.post.str.extractall(r'(\\#\\w+)')[0].value_counts().reset_index().values, columns=[\"hashtag\", \"count\"])\n",
    "Jan2017_hash_count = Jan2017_hash_count.sort_index(axis = 0, ascending=True)\n",
    "Jan2017_hash_count['hashtag'] = Jan2017_hash_count['hashtag'].replace({'#':''}, regex=True)\n",
    "Jan2017_hash_count.dtypes\n",
    "Jan2017_hash_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polair = pd.read_csv(\"raw/airPoland2017.csv\") # Source: http://powietrze.gios.gov.pl/pjp/archives\n",
    "                                                # Index source: https://airnow.gov/index.cfm?action=airnow.international\n",
    "polair['date'] = pd.to_datetime(polair['date'])\n",
    "\n",
    "polair = polair.replace(r',', '.', regex = True)\n",
    "polair.Bulwarowa = polair.Bulwarowa.replace(r' ', 'NaN', regex = True)\n",
    "\n",
    "polair.Bulwarowa = polair.Bulwarowa.astype(float)\n",
    "polair.Bujaka = polair.Bujaka.astype(float)\n",
    "polair['Aleja Krasińskiego'] = polair['Aleja Krasińskiego'].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "polair.dtypes\n",
    "polair_d = polair.resample('D', on='date').mean().reset_index()\n",
    "polair_d['avg'] = polair_d.mean(axis=1)\n",
    "polair_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jan2017Df = Jan2017.groupby(Jan2017['Days']).count()\n",
    "Jan2017Df = Jan2017Df['post']\n",
    "\n",
    "with plt.xkcd():\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    \n",
    "    Jan2017Df.plot(x='date', y='',kind=\"bar\",figsize=(20,10))\n",
    "    krakowCount.plot(kind='line',y='Krakowcount',color=\"green\",ax=ax)\n",
    "    WarszawaCount.plot(kind='line',y='Warszawacount',color=\"orange\",ax=ax)\n",
    "    polair_d.plot(kind='line',y='avg',color=\"red\",ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2017-01-08'\n",
    "end_date = '2017-01-12'\n",
    "mask = (Jan2017['date'] >= start_date) & (Jan2017['date'] <= end_date)\n",
    "Jan2017_filtered = Jan2017.loc[mask]\n",
    "print(sum(Jan2017_filtered.post.str.len()))\n",
    "Jan2017_filtered.head()\n",
    "\n",
    "Jan2017_f_h = pd.DataFrame(Jan2017_filtered.post.str.extractall(r'(\\#\\w+)')[0].value_counts().reset_index().values, columns=[\"hashtag\", \"count\"])\n",
    "Jan2017_f_h = Jan2017_f_h.sort_index(axis = 0, ascending=True)\n",
    "Jan2017_f_h['hashtag'] = Jan2017_f_h['hashtag'].replace({'#':''}, regex=True)\n",
    "Jan2017_f_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import Image, HTML\n",
    "\n",
    "def path_to_image_html(path):\n",
    "    '''\n",
    "     This function essentially convert the image url to \n",
    "     '<img src=\"'+ path + '\"/>' format. And one can put any\n",
    "     formatting adjustments to control the height, aspect ratio, size etc.\n",
    "     within as in the below example. \n",
    "    '''\n",
    "\n",
    "    return '<img src=\"'+ path + '\" style=max-height:124px;\"/>'\n",
    "\n",
    "HTML(dataDF.to_html(escape=False ,formatters=dict(image=path_to_image_html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries of posts mentioning 'Smog' from 2012 to 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countDF = dataDF.groupby(dataDF['Month-Year']).count()\n",
    "countDF = countDF['post']\n",
    "\n",
    "with plt.xkcd():\n",
    "    plt.axvspan(47,50, facecolor='skyblue', alpha=0.25)\n",
    "    plt.axvspan(35,38, facecolor='skyblue', alpha=0.25)\n",
    "    plt.axvspan(59,62, facecolor='skyblue', alpha=0.25)\n",
    "    plt.axvspan(71,74, facecolor='skyblue', alpha=0.25)\n",
    "    plt.axvspan(23,26, facecolor='skyblue', alpha=0.25)        \n",
    "    plt.axvspan(11,14, facecolor='skyblue', alpha=0.25)        \n",
    "    plt.axvspan(0,2, facecolor='skyblue', alpha=0.25)\n",
    " \n",
    "    countDF.plot(x='Month-Year', y='',kind=\"bar\",figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = dataDF.post.str.split(expand=True).stack().value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtranslate import translate\n",
    "\n",
    "hash_count = pd.DataFrame(dataDF.post.str.extractall(r'(\\#\\w+)')[0].value_counts().reset_index().values, columns=[\"hashtag\", \"count\"])\n",
    "hash_count = hash_count.sort_index(axis = 0, ascending=True)\n",
    "hash_count['hashtag'] = hash_count['hashtag'].replace({'#':''}, regex=True)\n",
    "hash_count.dtypes\n",
    "# hash_count['hashtag'] = mtranslate(hash_count['hashtag'],'en','pl')\n",
    "hash_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtranslate(data.post[5],'en','pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data['post'].str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "data.dtypes\n",
    "data.resample('Y', on='date').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from mtranslate import translate\n",
    "\n",
    "translation = []\n",
    "\n",
    "for postPl in range(0,7286):\n",
    "    \n",
    "    if data.post.str.len()[postPl] == 99999:\n",
    "    \n",
    "        sleep(randint(101,102))\n",
    "    \n",
    "    else:\n",
    "        clear_output(wait=True)\n",
    "        trans = mtranslate(data.post[postPl],'en','pl')\n",
    "        translation.append(trans)\n",
    "    \n",
    "    print(\"Current progress: \", np.round(postPl/len(data)*100,2),\"%\")\n",
    "    \n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in data.iterrows():\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Current progress: \", np.round(postPl/len(data)*100,2),\"%\")\n",
    "    \n",
    "%time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show image in dataframe: https://datascience.stackexchange.com/questions/38083/display-images-url-inside-pandas-dataframe\n",
    "\n",
    "https://stackoverflow.com/questions/37365824/pandas-ipython-notebook-include-and-display-an-image-in-a-dataframe\n",
    "\n",
    "Vader: https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f\n",
    "\n",
    "Simple WordCloud: http://kavita-ganesan.com/word-cloud-for-data-scientists/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
